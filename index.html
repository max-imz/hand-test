<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <title>Détection Main TensorFlow.js Handpose</title>
  <style>
    body, html {
      margin: 0; padding: 0; overflow: hidden;
      background: #111;
      color: #0ff;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      user-select: none;
      height: 100vh;
      justify-content: center;
    }
    h1 {
      margin: 0;
      font-weight: 600;
      font-size: 2rem;
      letter-spacing: 0.05em;
      text-shadow: 0 0 10px #0ff;
      user-select: text;
    }
    #video, #canvas {
      margin-top: 15px;
      border-radius: 12px;
      border: 2px solid #0ff;
      box-shadow: 0 0 20px #0ff44;
      transform: scaleX(-1);
      -webkit-transform: scaleX(-1);
      width: 640px;
      height: 480px;
      object-fit: cover;
      background-color: #222;
    }
  </style>
</head>
<body>
  <h1>Détection Main TensorFlow.js Handpose</h1>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas" width="640" height="480"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>

  <script>
    async function main() {
      const video = document.getElementById('video');
      const canvas = document.getElementById('canvas');
      const ctx = canvas.getContext('2d');

      // Accès à la caméra
      const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 } });
      video.srcObject = stream;
      await new Promise(resolve => video.onloadedmetadata = resolve);

      // Charger le modèle Handpose
      const model = await handpose.load();
      console.log('Modèle handpose chargé');

      function drawLandmarks(landmarks) {
        ctx.fillStyle = '#0ff';
        ctx.strokeStyle = '#08b';
        ctx.lineWidth = 2;

        // Connexions entre points (doigts)
        const fingers = [
          [0,1,2,3,4],
          [0,5,6,7,8],
          [0,9,10,11,12],
          [0,13,14,15,16],
          [0,17,18,19,20]
        ];

        fingers.forEach(finger => {
          ctx.beginPath();
          ctx.moveTo(landmarks[finger[0]][0], landmarks[finger[0]][1]);
          for(let i=1; i<finger.length; i++) {
            ctx.lineTo(landmarks[finger[i]][0], landmarks[finger[i]][1]);
          }
          ctx.stroke();
        });

        // Points des articulations
        landmarks.forEach(([x, y]) => {
          ctx.beginPath();
          ctx.arc(x, y, 6, 0, 2 * Math.PI);
          ctx.fill();
          ctx.stroke();
        });
      }

      async function detect() {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        const predictions = await model.estimateHands(video);

        if(predictions.length > 0) {
          predictions.forEach(prediction => {
            drawLandmarks(prediction.landmarks);
          });
        }

        requestAnimationFrame(detect);
      }

      detect();
    }

    main().catch(console.error);
  </script>
</body>
</html>
